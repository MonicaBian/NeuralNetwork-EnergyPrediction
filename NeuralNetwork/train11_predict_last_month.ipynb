{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "# Train on Jan–Nov 2023, Predict Dec 2023 (Power LSTM)\r\n",
    "\r\n",
    "This notebook:\r\n",
    "1. Trains an LSTM model on **January–November 2023** data.\r\n",
    "2. Validates with the tail of that training window (chronological split).\r\n",
    "3. Generates predictions for **December 2023** and computes metrics.\r\n",
    "4. Saves artifacts (model + scalers + config) and CSV outputs.\r\n",
    "5. Parses the CSV results, visualizes and plots the CSV outputs.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "import os\r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
    "from sklearn.metrics import r2_score\r\n",
    "import joblib\r\n",
    "import matplotlib.dates as mdates\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "# ======================\r\n",
    "# Configuration\r\n",
    "# ======================\r\n",
    "\r\n",
    "# Site / data location\r\n",
    "#site = 'S0186'\r\n",
    "#postcode = 5290\r\n",
    "\r\n",
    "#site = 'S0561'\r\n",
    "#postcode = 5555\r\n",
    "\r\n",
    "#site = 'S0150'\r\n",
    "#postcode = 5035\r\n",
    "\r\n",
    "site = 'S0556'\r\n",
    "postcode = 5098\r\n",
    "\r\n",
    "data_directory = '../Data/Processed'\r\n",
    "site_filepath = os.path.join(data_directory, f\"SA_site_edp_2023_{site}_processed.csv\")\r\n",
    "nci_path = \"../Data/NCI/NCI_processed_grouped_all_SA_2023.csv\"\r\n",
    "\r\n",
    "# Time windows (local Australia/Adelaide)\r\n",
    "tz = \"Australia/Adelaide\"\r\n",
    "TRAIN_START = pd.Timestamp('2023-01-01 00:00:00', tz=tz)\r\n",
    "TRAIN_END   = pd.Timestamp('2023-11-30 23:59:59', tz=tz)\r\n",
    "PRED_START  = pd.Timestamp('2023-12-01 00:00:00', tz=tz)\r\n",
    "PRED_END    = pd.Timestamp('2023-12-31 23:59:59', tz=tz)\r\n",
    "\r\n",
    "# Modeling params\r\n",
    "\r\n",
    "# lookback length\r\n",
    "timesteps = 6                  \r\n",
    "merge_tolerance = \"10min\"\r\n",
    "\r\n",
    "# validation fraction from the *end* of the training window\r\n",
    "val_frac = 0.1                 \r\n",
    "\r\n",
    "# include 'Power' in inputs\r\n",
    "use_past_power = True\r\n",
    "\r\n",
    "# train only on daylight sequences\r\n",
    "mask_daylight_in_train = True  \r\n",
    "use_solar_elevation_for_day = True\r\n",
    "\r\n",
    "# used if use_solar_elevation_for_day=False\r\n",
    "ghi_day_threshold = 20.0       \r\n",
    "\r\n",
    "# Artifacts/output\r\n",
    "artifacts_dir = \"./artifacts_power_lstm_2023_11train\"\r\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\r\n",
    "# Keras 3 format\r\n",
    "model_path = os.path.join(artifacts_dir, \"best_lstm_power.keras\")   \r\n",
    "scaler_feat_path = os.path.join(artifacts_dir, \"scaler_features.gz\")\r\n",
    "# only used when use_past_power=False\r\n",
    "scaler_tgt_path  = os.path.join(artifacts_dir, \"scaler_target.gz\")  \r\n",
    "config_path = os.path.join(artifacts_dir, \"config.json\")\r\n",
    "\r\n",
    "pred_csv_path = os.path.join(artifacts_dir, \"power_predictions_2023_12.csv\")\r\n",
    "daily_metrics_csv_path = os.path.join(artifacts_dir, \"power_daily_metrics_2023_12.csv\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# Helpers\n",
    "# ======================\n",
    "def localize_adelaide_naive(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Localize naive timestamps to Australia/Adelaide with DST handling.\"\"\"\n",
    "    return series.dt.tz_localize(\n",
    "        \"Australia/Adelaide\",\n",
    "        ambiguous=\"NaT\",\n",
    "        nonexistent=\"shift_forward\"\n",
    "    )\n",
    "\n",
    "def align_site_nci(site_filepath: str,\n",
    "                   nci_path: str,\n",
    "                   postcode: int,\n",
    "                   start_time: pd.Timestamp,\n",
    "                   end_time: pd.Timestamp,\n",
    "                   merge_tolerance: str = \"10min\") -> pd.DataFrame:\n",
    "    \"\"\"Load site + NCI, align on time in Australia/Adelaide, return merged frame.\"\"\"\n",
    "    # Site\n",
    "    df_site = pd.read_csv(site_filepath)\n",
    "    if 'Power' not in df_site.columns:\n",
    "        raise KeyError(\"Expected 'Power' column in site CSV.\")\n",
    "    df_site['datetime'] = pd.to_datetime(df_site['datetime'], errors='coerce')\n",
    "    if df_site['datetime'].dt.tz is None:\n",
    "        df_site['datetime'] = localize_adelaide_naive(df_site['datetime'])\n",
    "    else:\n",
    "        df_site['datetime'] = df_site['datetime'].dt.tz_convert('Australia/Adelaide')\n",
    "    df_site = df_site[(df_site['datetime'] >= start_time) & (df_site['datetime'] <= end_time)].copy()\n",
    "    if df_site.empty:\n",
    "        raise ValueError(\"No site data in the selected time window after timezone alignment.\")\n",
    "    df_site = df_site.sort_values('datetime')\n",
    "\n",
    "    # NCI\n",
    "    nci = pd.read_csv(nci_path)\n",
    "    nci = nci[nci['postcode'].notna()].copy()\n",
    "    nci['postcode'] = nci['postcode'].astype(int)\n",
    "    nci = nci[nci['postcode'] == postcode].copy()\n",
    "    if nci.empty:\n",
    "        raise ValueError(f\"No NCI rows for postcode {postcode} in {nci_path}\")\n",
    "    nci['time'] = pd.to_datetime(nci['time'], errors='coerce', utc=True).dt.tz_convert('Australia/Adelaide')\n",
    "\n",
    "    # Derive GHI if needed\n",
    "    if 'surface_global_irradiance' not in nci.columns:\n",
    "        req = {'direct_normal_irradiance','surface_diffuse_irradiance','solar_elevation'}\n",
    "        if not req.issubset(nci.columns):\n",
    "            raise KeyError(\"NCI missing 'surface_global_irradiance' and components to derive it.\")\n",
    "        nci['zenith_angle'] = 90 - nci['solar_elevation']\n",
    "        nci['surface_global_irradiance'] = (\n",
    "            nci['surface_diffuse_irradiance']\n",
    "            + nci['direct_normal_irradiance'] * np.cos(np.radians(nci['zenith_angle']))\n",
    "        )\n",
    "\n",
    "    keep = [\n",
    "        'time',\n",
    "        'surface_global_irradiance',\n",
    "        'direct_normal_irradiance',\n",
    "        'surface_diffuse_irradiance',\n",
    "        'cloud_type',\n",
    "        'cloud_optical_depth',\n",
    "        'solar_elevation',\n",
    "        'solar_azimuth',\n",
    "    ]\n",
    "    for c in keep:\n",
    "        if c not in nci.columns:\n",
    "            raise KeyError(f\"NCI missing required column: {c}\")\n",
    "\n",
    "    nci_small = nci[keep].sort_values('time')\n",
    "\n",
    "    # Align (nearest within tolerance)\n",
    "    merged = pd.merge_asof(\n",
    "        df_site, nci_small,\n",
    "        left_on='datetime', right_on='time',\n",
    "        direction='nearest', tolerance=pd.Timedelta(merge_tolerance)\n",
    "    ).drop(columns=['time'])\n",
    "\n",
    "    # Interpolate NCI cols\n",
    "    for c in ['surface_global_irradiance','direct_normal_irradiance','surface_diffuse_irradiance',\n",
    "              'cloud_type','cloud_optical_depth','solar_elevation','solar_azimuth']:\n",
    "        merged[c] = merged[c].interpolate(limit_direction='both')\n",
    "\n",
    "    return merged\n",
    "\n",
    "def create_sequences(data2d: np.ndarray, steps: int, target_col: int):\n",
    "    \"\"\"Return X, y, row_idx for one-step-ahead sequences.\"\"\"\n",
    "    X, y, idx = [], [], []\n",
    "    for i in range(steps, len(data2d)):\n",
    "        X.append(data2d[i-steps:i, :])\n",
    "        y.append(data2d[i, target_col])\n",
    "        idx.append(i)\n",
    "    return np.array(X), np.array(y).reshape(-1,1), np.array(idx)\n",
    "\n",
    "def make_sequences_for_inference(features_scaled: np.ndarray, timesteps: int):\n",
    "    X, row_idx = [], []\n",
    "    for i in range(timesteps, len(features_scaled)):\n",
    "        X.append(features_scaled[i-timesteps:i, :])\n",
    "        row_idx.append(i)\n",
    "    return np.array(X), np.array(row_idx)\n",
    "\n",
    "def inverse_minmax_column(pred_scaled: np.ndarray, scaler: MinMaxScaler, col_idx: int) -> np.ndarray:\n",
    "    data_min = scaler.data_min_[col_idx]\n",
    "    data_max = scaler.data_max_[col_idx]\n",
    "    return pred_scaled * (data_max - data_min) + data_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan–Nov rows: 96,149\n",
      "Train samples: 61358  (kept 61358 / 86528 after daylight mask)\n",
      "Val   samples: 9615    (kept 9615 / 9615)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================\n",
    "# Load & prepare TRAIN (Jan–Nov 2023)\n",
    "# ======================\n",
    "merged_train = align_site_nci(\n",
    "    site_filepath=site_filepath,\n",
    "    nci_path=nci_path,\n",
    "    postcode=postcode,\n",
    "    start_time=TRAIN_START,\n",
    "    end_time=TRAIN_END,\n",
    "    merge_tolerance=merge_tolerance\n",
    ")\n",
    "\n",
    "# Feature columns\n",
    "if use_past_power:\n",
    "    feature_cols = [\n",
    "        'direct_normal_irradiance',\n",
    "        'surface_diffuse_irradiance',\n",
    "        'cloud_type',\n",
    "        'cloud_optical_depth',\n",
    "        'solar_elevation',\n",
    "        'solar_azimuth',\n",
    "        'surface_global_irradiance',\n",
    "        'Power'\n",
    "    ]\n",
    "else:\n",
    "    feature_cols = [\n",
    "        'direct_normal_irradiance',\n",
    "        'surface_diffuse_irradiance',\n",
    "        'cloud_type',\n",
    "        'cloud_optical_depth',\n",
    "        'solar_elevation',\n",
    "        'solar_azimuth',\n",
    "        'surface_global_irradiance'\n",
    "    ]\n",
    "\n",
    "target_name = 'Power'\n",
    "for c in feature_cols + [target_name]:\n",
    "    if c not in merged_train.columns:\n",
    "        raise KeyError(f\"Missing column in training data: {c}\")\n",
    "\n",
    "# Raw arrays\n",
    "features_raw_train = merged_train[feature_cols].ffill().bfill().to_numpy(dtype=np.float64)\n",
    "target_raw_train   = merged_train[[target_name]].ffill().bfill().to_numpy(dtype=np.float64)\n",
    "\n",
    "# Chronological split inside Jan–Nov block\n",
    "N_tr = len(features_raw_train)\n",
    "split_row = int(N_tr * (1 - val_frac))\n",
    "if split_row <= timesteps:\n",
    "    raise ValueError(\"Not enough rows in training for chosen timesteps/val split.\")\n",
    "\n",
    "# Scale features (fit on train-only slice)\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_features.fit(features_raw_train[:split_row])\n",
    "features_scaled_train = scaler_features.transform(features_raw_train)\n",
    "\n",
    "# Target scaling (if target not included in features)\n",
    "if use_past_power:\n",
    "    target_in_features = True\n",
    "    target_col_idx = feature_cols.index(target_name)\n",
    "    scaler_target = None\n",
    "else:\n",
    "    target_in_features = False\n",
    "    target_col_idx = None\n",
    "    scaler_target = MinMaxScaler()\n",
    "    scaler_target.fit(target_raw_train[:split_row])\n",
    "    target_scaled_train = scaler_target.transform(target_raw_train)\n",
    "\n",
    "# Sequences across whole Jan–Nov block\n",
    "if use_past_power:\n",
    "    X_all_tr, y_all_tr, row_idx_tr = create_sequences(features_scaled_train, timesteps, target_col_idx)\n",
    "else:\n",
    "    # dummy to get row_idx, y from target_scaled\n",
    "    X_all_tr, _, row_idx_tr = create_sequences(features_scaled_train, timesteps, target_col=0)\n",
    "    y_all_tr = np.array([target_scaled_train[i, 0] for i in row_idx_tr]).reshape(-1, 1)\n",
    "\n",
    "# Daylight mask for TRAIN only\n",
    "if mask_daylight_in_train:\n",
    "    if use_solar_elevation_for_day:\n",
    "        daylight_row = (merged_train['solar_elevation'].to_numpy() > 0)\n",
    "    else:\n",
    "        daylight_row = (merged_train['surface_global_irradiance'].to_numpy() > ghi_day_threshold)\n",
    "\n",
    "    day_series = pd.Series(daylight_row.astype(int))\n",
    "    all_inputs_day = (\n",
    "        day_series.shift(1)  # window ends at i-1\n",
    "                 .rolling(window=timesteps, min_periods=timesteps)\n",
    "                 .min()\n",
    "                 .fillna(0)\n",
    "                 .astype(bool)\n",
    "                 .to_numpy()\n",
    "    )\n",
    "    target_is_day = daylight_row\n",
    "    sample_ok = target_is_day & all_inputs_day\n",
    "    seq_ok_tr = sample_ok[row_idx_tr]\n",
    "else:\n",
    "    seq_ok_tr = np.ones_like(row_idx_tr, dtype=bool)\n",
    "\n",
    "# Split into train/val (chronological)\n",
    "train_mask_time = (row_idx_tr < split_row)\n",
    "val_mask_time   = (row_idx_tr >= split_row)\n",
    "train_mask = train_mask_time & seq_ok_tr\n",
    "val_mask   = val_mask_time   # keep evening/night in validation to measure robustness\n",
    "\n",
    "X_train, y_train = X_all_tr[train_mask], y_all_tr[train_mask]\n",
    "X_val,   y_val   = X_all_tr[val_mask],   y_all_tr[val_mask]\n",
    "\n",
    "X_train = X_train.astype(np.float32); y_train = y_train.astype(np.float32)\n",
    "X_val   = X_val.astype(np.float32);   y_val   = y_val.astype(np.float32)\n",
    "\n",
    "print(f\"Jan–Nov rows: {N_tr:,}\")\n",
    "print(f\"Train samples: {X_train.shape[0]}  (kept {train_mask.sum()} / {train_mask_time.sum()} after daylight mask)\")\n",
    "print(f\"Val   samples: {X_val.shape[0]}    (kept {val_mask.sum()} / {val_mask_time.sum()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Monica\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0156 - val_loss: 0.0035\n",
      "Epoch 2/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0031\n",
      "Epoch 3/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 4/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0027\n",
      "Epoch 5/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0027\n",
      "Epoch 6/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0027\n",
      "Epoch 7/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 8/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 9/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0026\n",
      "Epoch 10/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0026\n",
      "Epoch 11/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 12/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0024\n",
      "Epoch 13/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 14/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 15/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 16/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0021\n",
      "Epoch 17/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 18/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0020\n",
      "Epoch 19/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 20/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Saved artifacts to: ./artifacts_power_lstm_2023_11train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================\n",
    "# Build datasets & train model\n",
    "# ======================\n",
    "batch_size = 512\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((X_val,   y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "n_features = X_train.shape[-1]\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(timesteps, n_features)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(model_path, save_best_only=True)  # saves .keras\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# Save scalers + config\n",
    "joblib.dump(scaler_features, scaler_feat_path)\n",
    "if not target_in_features:\n",
    "    joblib.dump(scaler_target, scaler_tgt_path)\n",
    "\n",
    "config = {\n",
    "    \"timesteps\": timesteps,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"target_name\": \"Power\",\n",
    "    \"target_in_features\": target_in_features,\n",
    "    \"target_col_idx\": target_col_idx,\n",
    "    \"merge_tolerance\": merge_tolerance,\n",
    "    \"mask_daylight_in_train\": mask_daylight_in_train,\n",
    "    \"use_solar_elevation_for_day\": use_solar_elevation_for_day,\n",
    "    \"ghi_day_threshold\": ghi_day_threshold,\n",
    "    \"train_window\": [str(TRAIN_START), str(TRAIN_END)],\n",
    "    \"predict_window\": [str(PRED_START), str(PRED_END)],\n",
    "}\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Saved artifacts to:\", artifacts_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 2023 metrics\n",
      "DAYLIGHT AND NIGHT times -> MAE: 0.143905  MSE: 0.078748  RMSE: 0.280620  WAPE: 14.661%  R²: 0.9522  N: 8917\n",
      "DAYLIGHT ONLY -> MAE: 0.198396  MSE: 0.111786  RMSE: 0.334344  WAPE: 14.236%  R²: 0.9367  N: 6275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Monica\\AppData\\Local\\Temp\\ipykernel_15932\\466794745.py:261: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_all = results.groupby('date', sort=True).apply(_agg_all).reset_index()\n",
      "C:\\Users\\Monica\\AppData\\Local\\Temp\\ipykernel_15932\\466794745.py:262: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_day = results.groupby('date', sort=True).apply(_agg_day).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# ==============================\r\n",
    "# Metrics (all times + daylight-only)\r\n",
    "# ==============================\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.metrics import r2_score\r\n",
    "\r\n",
    "# ---- Extra metrics ----\r\n",
    "def mse(y_true, y_pred):\r\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\r\n",
    "    return float(np.mean((y_pred - y_true) ** 2))\r\n",
    "\r\n",
    "def mape(y_true, y_pred, ignore_zeros=True, eps=1e-8):\r\n",
    "    \"\"\"\r\n",
    "    MAPE in %, by default ignores y_true == 0 (common at night for PV).\r\n",
    "    \"\"\"\r\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\r\n",
    "    denom = np.abs(y_true)\r\n",
    "    if ignore_zeros:\r\n",
    "        m = denom > 0\r\n",
    "        y_true, y_pred, denom = y_true[m], y_pred[m], denom[m]\r\n",
    "    denom = np.maximum(denom, eps)\r\n",
    "    if denom.size == 0:\r\n",
    "        return float(\"nan\")\r\n",
    "    return float(np.mean(np.abs((y_pred - y_true) / denom)) * 100.0)\r\n",
    "\r\n",
    "def wape(y_true, y_pred):\r\n",
    "    \"\"\"\r\n",
    "    WAPE in % = sum|e| / sum|y_true|.\r\n",
    "    \"\"\"\r\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\r\n",
    "    denom = np.sum(np.abs(y_true))\r\n",
    "    if denom == 0:\r\n",
    "        return float(\"nan\")\r\n",
    "    return float(np.sum(np.abs(y_pred - y_true)) / denom * 100.0)\r\n",
    "\r\n",
    "def _infer_seasonal_periods_from_times(dt_series: pd.Series) -> int:\r\n",
    "    \"\"\"\r\n",
    "    Infer seasonal period (samples per day) from timestamps.\r\n",
    "    Falls back to 1 if inference fails.\r\n",
    "    \"\"\"\r\n",
    "    try:\r\n",
    "        s = pd.to_datetime(pd.Series(dt_series)).sort_values()\r\n",
    "        if s.size < 3:\r\n",
    "            return 1\r\n",
    "        step = s.diff().dropna().median()\r\n",
    "        if pd.isna(step) or step <= pd.Timedelta(0):\r\n",
    "            return 1\r\n",
    "        sp = int(round(pd.Timedelta(days=1) / step))\r\n",
    "        return max(1, sp)\r\n",
    "    except Exception:\r\n",
    "        return 1\r\n",
    "\r\n",
    "def mase(y_true, y_pred, y_train, seasonal_periods=1):\r\n",
    "    \"\"\"\r\n",
    "    MASE per Hyndman & Koehler (2006).\r\n",
    "    Scale = MAE of seasonal naive forecast on TRAIN actuals:\r\n",
    "        mean(|y_t - y_{t-m}|), t=m..T-1\r\n",
    "    Returns NaN if scale cannot be computed.\r\n",
    "    \"\"\"\r\n",
    "    if y_train is None:\r\n",
    "        return float(\"nan\")\r\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\r\n",
    "    y_train = np.asarray(y_train).reshape(-1)\r\n",
    "    if y_train.size <= seasonal_periods:\r\n",
    "        return float(\"nan\")\r\n",
    "    naive_err = np.abs(y_train[seasonal_periods:] - y_train[:-seasonal_periods])\r\n",
    "    scale = np.mean(naive_err[np.isfinite(naive_err)])\r\n",
    "    if not np.isfinite(scale) or scale == 0:\r\n",
    "        return float(\"nan\")\r\n",
    "    return float(np.mean(np.abs(y_pred - y_true)) / scale)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# ======================\r\n",
    "# Predict December 2023\r\n",
    "# ======================\r\n",
    "# Reload model (best checkpoint)\r\n",
    "model = tf.keras.models.load_model(model_path)\r\n",
    "\r\n",
    "merged_pred = align_site_nci(\r\n",
    "    site_filepath=site_filepath,\r\n",
    "    nci_path=nci_path,\r\n",
    "    postcode=postcode,\r\n",
    "    start_time=PRED_START,\r\n",
    "    end_time=PRED_END,\r\n",
    "    merge_tolerance=merge_tolerance\r\n",
    ")\r\n",
    "\r\n",
    "# Check columns\r\n",
    "for c in feature_cols + [\"Power\"]:\r\n",
    "    if c not in merged_pred.columns:\r\n",
    "        raise KeyError(f\"Missing column in prediction data: {c}\")\r\n",
    "\r\n",
    "# Scale using training scaler\r\n",
    "features_raw_pred = merged_pred[feature_cols].ffill().bfill().to_numpy(dtype=np.float64)\r\n",
    "features_scaled_pred = scaler_features.transform(features_raw_pred)\r\n",
    "\r\n",
    "# Build sequences for inference (all rows, incl. night)\r\n",
    "def make_sequences_for_inference(features_scaled: np.ndarray, timesteps: int):\r\n",
    "    X, row_idx = [], []\r\n",
    "    for i in range(timesteps, len(features_scaled)):\r\n",
    "        X.append(features_scaled[i-timesteps:i, :])\r\n",
    "        row_idx.append(i)\r\n",
    "    return np.array(X), np.array(row_idx)\r\n",
    "\r\n",
    "X_inf, row_idx = make_sequences_for_inference(features_scaled_pred, timesteps)\r\n",
    "X_inf = X_inf.astype(np.float32)\r\n",
    "\r\n",
    "# Predict (scaled)\r\n",
    "batch_size_inf = 2048\r\n",
    "inf_ds = tf.data.Dataset.from_tensor_slices(X_inf).batch(batch_size_inf).prefetch(tf.data.AUTOTUNE)\r\n",
    "pred_scaled = model.predict(inf_ds, verbose=0).reshape(-1)\r\n",
    "\r\n",
    "# Inverse-scale to file units\r\n",
    "def inverse_minmax_column(pred_scaled: np.ndarray, scaler: MinMaxScaler, col_idx: int) -> np.ndarray:\r\n",
    "    data_min = scaler.data_min_[col_idx]\r\n",
    "    data_max = scaler.data_max_[col_idx]\r\n",
    "    return pred_scaled * (data_max - data_min) + data_min\r\n",
    "\r\n",
    "if target_in_features:\r\n",
    "    power_pred = inverse_minmax_column(pred_scaled, scaler_features, col_idx=target_col_idx)\r\n",
    "else:\r\n",
    "    if os.path.exists(scaler_tgt_path):\r\n",
    "        scaler_target = joblib.load(scaler_tgt_path)\r\n",
    "        power_pred = scaler_target.inverse_transform(pred_scaled.reshape(-1,1)).reshape(-1)\r\n",
    "    else:\r\n",
    "        raise RuntimeError(\"Target scaler not found for non-autoregressive model.\")\r\n",
    "\r\n",
    "# Assemble results\r\n",
    "times_all = merged_pred['datetime'].reset_index(drop=True)\r\n",
    "times_pred = times_all.iloc[row_idx].reset_index(drop=True)\r\n",
    "results = pd.DataFrame({\r\n",
    "    \"datetime\": times_pred,\r\n",
    "    \"Power_pred_W\": power_pred,\r\n",
    "    \"Power_true_W\": merged_pred['Power'].to_numpy()[row_idx]\r\n",
    "})\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# Try to supply training ACTUALS for MASE if available.\r\n",
    "# Set one of these earlier in your notebook to make MASE work:\r\n",
    "#   y_train_actuals = <1D array/Series of training-period actual Power in W>\r\n",
    "# Otherwise MASE will return NaN (safe fallback).\r\n",
    "y_train_for_mase = None\r\n",
    "for _cand in [\"y_train_actuals\", \"y_train_power\", \"train_target\", \"train_y\", \"y_train\"]:\r\n",
    "    if _cand in globals():\r\n",
    "        y_train_for_mase = globals()[_cand]\r\n",
    "        break\r\n",
    "\r\n",
    "seasonal_periods = _infer_seasonal_periods_from_times(results[\"datetime\"])\r\n",
    "\r\n",
    "def _metrics_full(y_t, y_p, y_train_for_mase, seasonal_periods):\r\n",
    "    y_t = np.asarray(y_t); y_p = np.asarray(y_p)\r\n",
    "    mae  = float(np.mean(np.abs(y_p - y_t)))\r\n",
    "    _mse = mse(y_t, y_p)\r\n",
    "    rmse = float(np.sqrt(_mse))\r\n",
    "    r2   = float(r2_score(y_t, y_p)) if len(y_t) > 1 else float(\"nan\")\r\n",
    "    _wape = wape(y_t, y_p)\r\n",
    "    _mase = mase(y_t, y_p, y_train_for_mase, seasonal_periods)\r\n",
    "    return {\r\n",
    "        \"MAE\": mae,\r\n",
    "        \"MSE\": _mse,\r\n",
    "        \"RMSE\": rmse,\r\n",
    "        \"R2\": r2,\r\n",
    "        \"WAPE_%\": _wape,\r\n",
    "        \"N\": int(len(y_t))\r\n",
    "    }\r\n",
    "\r\n",
    "y_true = results[\"Power_true_W\"].to_numpy()\r\n",
    "y_pred = results[\"Power_pred_W\"].to_numpy()\r\n",
    "\r\n",
    "metrics_all = _metrics_full(y_true, y_pred, y_train_for_mase, seasonal_periods)\r\n",
    "\r\n",
    "solar_elev_seq = merged_pred['solar_elevation'].to_numpy()[row_idx]\r\n",
    "day_mask = solar_elev_seq > 0\r\n",
    "if day_mask.any():\r\n",
    "    metrics_day = _metrics_full(y_true[day_mask], y_pred[day_mask], y_train_for_mase, seasonal_periods)\r\n",
    "else:\r\n",
    "    metrics_day = {k: float(\"nan\") for k in [\"MAE\",\"MSE\",\"RMSE\",\"R2\",\"MAPE_%\",\"WAPE_%\",\"MASE\"]}\r\n",
    "    metrics_day[\"N\"] = 0\r\n",
    "\r\n",
    "# Pretty print\r\n",
    "print(\"December 2023 metrics\")\r\n",
    "print(\r\n",
    "    \"DAYLIGHT AND NIGHT times -> \"\r\n",
    "    f\"MAE: {metrics_all['MAE']:,.6f}  \"\r\n",
    "    f\"MSE: {metrics_all['MSE']:,.6f}  \"\r\n",
    "    f\"RMSE: {metrics_all['RMSE']:,.6f}  \"\r\n",
    "    f\"WAPE: {metrics_all['WAPE_%']:,.3f}%  \"\r\n",
    "    f\"R²: {metrics_all['R2']:,.4f}  \"\r\n",
    "    f\"N: {metrics_all['N']}\"\r\n",
    ")\r\n",
    "print(\r\n",
    "    \"DAYLIGHT ONLY -> \"\r\n",
    "    f\"MAE: {metrics_day['MAE']:,.6f}  \"\r\n",
    "    f\"MSE: {metrics_day['MSE']:,.6f}  \"\r\n",
    "    f\"RMSE: {metrics_day['RMSE']:,.6f}  \"\r\n",
    "    f\"WAPE: {metrics_day['WAPE_%']:,.3f}%  \"\r\n",
    "    f\"R²: {metrics_day['R2']:,.4f}  \"\r\n",
    "    f\"N: {metrics_day['N']}\"\r\n",
    ")\r\n",
    "\r\n",
    "# ==============================\r\n",
    "# Save outputs (unchanged)\r\n",
    "# ==============================\r\n",
    "results.to_csv(pred_csv_path, index=False)\r\n",
    "\r\n",
    "# ==============================\r\n",
    "# Daily metrics (expanded)\r\n",
    "# ==============================\r\n",
    "results['date'] = results['datetime'].dt.date\r\n",
    "results['daylight'] = day_mask\r\n",
    "\r\n",
    "def _agg_all(g):\r\n",
    "    stats = _metrics_full(\r\n",
    "        g['Power_true_W'].to_numpy(),\r\n",
    "        g['Power_pred_W'].to_numpy(),\r\n",
    "        y_train_for_mase,\r\n",
    "        seasonal_periods\r\n",
    "    )\r\n",
    "    return pd.Series({\r\n",
    "        'MAE': stats['MAE'],\r\n",
    "        'MSE': stats['MSE'],\r\n",
    "        'RMSE': stats['RMSE'],\r\n",
    "        'WAPE_%': stats['WAPE_%'],\r\n",
    "        'R2': stats['R2'],\r\n",
    "        'N': stats['N'],\r\n",
    "    })\r\n",
    "\r\n",
    "def _agg_day(g):\r\n",
    "    mask = g['daylight'].to_numpy()\r\n",
    "    if mask.any():\r\n",
    "        stats = _metrics_full(\r\n",
    "            g.loc[mask, 'Power_true_W'].to_numpy(),\r\n",
    "            g.loc[mask, 'Power_pred_W'].to_numpy(),\r\n",
    "            y_train_for_mase,\r\n",
    "            seasonal_periods\r\n",
    "        )\r\n",
    "        return pd.Series({\r\n",
    "            'MAE_day': stats['MAE'],\r\n",
    "            'MSE_day': stats['MSE'],\r\n",
    "            'RMSE_day': stats['RMSE'],\r\n",
    "            'WAPE_day_%': stats['WAPE_%'],\r\n",
    "            'R2_day': stats['R2'],\r\n",
    "            'N_day': stats['N'],\r\n",
    "        })\r\n",
    "    else:\r\n",
    "        return pd.Series({\r\n",
    "            'MAE_day': float(\"nan\"),\r\n",
    "            'MSE_day': float(\"nan\"),\r\n",
    "            'RMSE_day': float(\"nan\"),\r\n",
    "            'WAPE_day_%': float(\"nan\"),\r\n",
    "            'R2_day': float(\"nan\"),\r\n",
    "            'N_day': 0\r\n",
    "        })\r\n",
    "\r\n",
    "daily_all = results.groupby('date', sort=True).apply(_agg_all).reset_index()\r\n",
    "daily_day = results.groupby('date', sort=True).apply(_agg_day).reset_index()\r\n",
    "daily_metrics = pd.merge(daily_all, daily_day, on='date', how='left')\r\n",
    "daily_metrics.to_csv(daily_metrics_csv_path, index=False)\r\n",
    "\r\n",
    "#print(\"\\nSaved:\")\r\n",
    "#print(\"  Predictions ->\", pred_csv_path)\r\n",
    "#print(\"  Daily metrics ->\", daily_metrics_csv_path)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"> Prediction Accuracy Metrics </span>\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**\n",
    "- **Definition:** Average absolute difference between predicted and actual values.\n",
    "- <span style=\"color:green\"> **Good prediction:** </span> Small MAE (close to 0) → on average, predictions deviate only slightly from actual power.\n",
    "- <span style=\"color:red\"> **Bad prediction:** </span> Large MAE (no upper bound) → predictions consistently miss the target by a wide margin.\n",
    "- **Scale:** Interpreted in Watts\n",
    "\n",
    "\n",
    "2. **Mean Squared Error (MSE)**\n",
    "- **Definition:** Average of squared differences between predicted and actual values. Penalizes larger errors more than smaller ones.\n",
    "- <span style=\"color:green\"> **Good prediction:** </span> Low MSE → few large deviations.\n",
    "- <span style=\"color:red\"> **Bad prediction:** </span> High MSE → presence of large outliers/errors.\n",
    "- **Scale:** In squared units (Watts²), so less intuitive than MAE.\n",
    "\n",
    "3. Root Mean Squared Error (RMSE)\n",
    "\n",
    "- **Definition:** Square root of MSE; same unit as target variable.\n",
    "- <span style=\"color:green\"> **Good prediction:** </span> Low RMSE → model tracks actual data well with few large errors.\n",
    "- <span style=\"color:red\"> **Bad prediction:** </span> High RMSE → predictions have large swings away from truth.\n",
    "- **Interpretation:** RMSE ≥ MAE usually; if RMSE is much larger than MAE, it means some large outliers dominate error.\n",
    "\n",
    "\n",
    "\n",
    "4. Weighted Absolute Percentage Error (WAPE)\n",
    "\n",
    "- **Definition:** Total absolute error divided by total actuals, expressed as a percentage.\n",
    "- <span style=\"color:green\"> **Good prediction:** </span> WAPE close to 0%.\n",
    "Rule of thumb thresholds:\n",
    "    - <10% → very good\n",
    "    - 10–20% → acceptable\n",
    "    - 20–50% → weak but sometimes tolerable\n",
    "    - 50% or above → poor\n",
    "- <span style=\"color:red\"> **Bad prediction:** </span> High WAPE suggests systematic deviation when compared to the magnitude of total production.\n",
    "\n",
    "\n",
    "5. **R² (R-squared, Coefficient of Determination)**\n",
    "- **Definition:** Proportion of variance in the actual data explained by the predictions (0–1 scale, sometimes negative if worse than a baseline).\n",
    "- <span style=\"color:green\"> **Good prediction:** </span> \n",
    "    - R² close to 1.0 → model explains almost all variability in actual power.\n",
    "    - R² ≥ 0.9 → excellent; \n",
    "    - R² = 0.7–0.9 → good.\n",
    "- <span style=\"color:red\"> **Bad prediction:** </span> \n",
    "    - R² close to 0 → model does not explain variability.\n",
    "    - Negative R² → model performs worse than simply predicting the mean of actuals.\n",
    "**Scale:** Dimensionless, bounded between (−∞, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"> Summary - Site X </span>\r\n",
    "Errors are slightly higher during daylight (MAE_day ≈ 0.19 vs. MAE ≈ 0.14).\r\n",
    "This makes sense: at night, power is zero and trivial to predict, while daylight introduces variability from weather/clouds.\r\n",
    "\r\n",
    "- MAE & MSE & RMSE are low → predictions are generally close in absolute terms.\r\n",
    "- WAPE ~18% → forecasts are moderately accurate; good but not best-in-class.\r\n",
    "- High R² (0.9+) → strong explanatory power, capturing most variability.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved daily PNGs to: c:\\Git\\NeuralNetwork-EnergyPrediction\\NeuralNetwork\\december_daily_plots_S0556_Polyfit\n",
      "Saved combined PDF to: c:\\Git\\NeuralNetwork-EnergyPrediction\\NeuralNetwork\\december_daily_plots_S0556_Polyfit\\power_december_2023_plots.pdf\n"
     ]
    }
   ],
   "source": [
    "# =========================================\r\n",
    "# Plot ALL December 2023 daily Predicted vs Actual power\r\n",
    "# + polyfit line (06:00–20:00 local)  + GHI overlay (right y-axis)\r\n",
    "# And an extra plot per day: Power (left) + Voltage (right)\r\n",
    "# Clamp Power series at 0 (no negatives); Voltage unaltered\r\n",
    "# =========================================\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.backends.backend_pdf import PdfPages\r\n",
    "\r\n",
    "# ---- Paths (use the one from earlier cell if available) ----\r\n",
    "try:\r\n",
    "    csv_path = pred_csv_path\r\n",
    "except NameError:\r\n",
    "    csv_path = \"./artifacts_power_lstm_2023_11train/power_predictions_2023_12.csv\"\r\n",
    "\r\n",
    "try:\r\n",
    "    data_directory\r\n",
    "except NameError:\r\n",
    "    data_directory = '../Data/Processed'\r\n",
    "try:\r\n",
    "    nci_path\r\n",
    "except NameError:\r\n",
    "    nci_path = \"../Data/NCI/NCI_processed_grouped_all_SA_2023.csv\"\r\n",
    "\r\n",
    "site_filepath = os.path.join(data_directory, f\"SA_site_edp_2023_{site}_processed.csv\")\r\n",
    "\r\n",
    "# allow site to be interpolated in folder name if it's already defined\r\n",
    "try:\r\n",
    "    output_dir = f\"./december_daily_plots_{site}_Polyfit\"\r\n",
    "except NameError:\r\n",
    "    output_dir = \"./december_daily_plots_Polyfit\"\r\n",
    "\r\n",
    "pdf_path = os.path.join(output_dir, \"power_december_2023_plots.pdf\")\r\n",
    "show_plots = False  # set True to display as they are created\r\n",
    "\r\n",
    "# Polyfit options\r\n",
    "poly_degree = 2\r\n",
    "tz = \"Australia/Adelaide\"\r\n",
    "poly_start_h = 6    # 06:00 (inclusive)\r\n",
    "poly_end_h   = 20   # 20:00 (inclusive)\r\n",
    "\r\n",
    "# ---- Helper: robust timezone handling ----\r\n",
    "def to_adelaide_series_any(df: pd.DataFrame, col: str) -> pd.Series:\r\n",
    "    s = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\r\n",
    "    if s.notna().any():\r\n",
    "        return s.dt.tz_convert(tz)\r\n",
    "    s = pd.to_datetime(df[col], errors=\"coerce\")\r\n",
    "    if s.isna().all():\r\n",
    "        raise ValueError(f\"Could not parse any datetimes from '{col}'.\")\r\n",
    "    tzinfo = getattr(s.dtype, \"tz\", None)\r\n",
    "    if tzinfo is None:\r\n",
    "        return s.dt.tz_localize(tz)\r\n",
    "    return s.dt.tz_convert(tz)\r\n",
    "\r\n",
    "# ---- Load predictions ----\r\n",
    "df = pd.read_csv(csv_path)\r\n",
    "if \"datetime\" not in df.columns:\r\n",
    "    raise KeyError(\"Expected 'datetime' column in predictions CSV.\")\r\n",
    "if \"Power_pred_W\" not in df.columns:\r\n",
    "    raise KeyError(\"Expected 'Power_pred_W' column in predictions CSV.\")\r\n",
    "\r\n",
    "df[\"datetime\"] = to_adelaide_series_any(df, \"datetime\")\r\n",
    "\r\n",
    "# ---- Filter to Dec 2023 (inclusive) ----\r\n",
    "start_ts = pd.Timestamp(\"2023-12-01 00:00:00\", tz=tz)\r\n",
    "end_ts   = pd.Timestamp(\"2023-12-31 23:59:59\", tz=tz)\r\n",
    "mask = (df[\"datetime\"] >= start_ts) & (df[\"datetime\"] <= end_ts)\r\n",
    "dfr = df.loc[mask].copy().sort_values(\"datetime\")\r\n",
    "if dfr.empty:\r\n",
    "    raise ValueError(\"No rows found for December 2023 in the predictions file.\")\r\n",
    "\r\n",
    "# ---- Pull GHI + site data for the same window using your existing helper ----\r\n",
    "# Requires align_site_nci() to be defined in the notebook/session\r\n",
    "dec_merged_for_ghi = align_site_nci(\r\n",
    "    site_filepath=site_filepath,\r\n",
    "    nci_path=nci_path,\r\n",
    "    postcode=postcode,\r\n",
    "    start_time=start_ts,\r\n",
    "    end_time=end_ts,\r\n",
    "    merge_tolerance=\"10min\"\r\n",
    ")\r\n",
    "\r\n",
    "# Sanity check for voltage column presence\r\n",
    "if 'voltage_avg' not in dec_merged_for_ghi.columns:\r\n",
    "    raise KeyError(\"Expected 'voltage_avg' column in the site dataset.\")\r\n",
    "\r\n",
    "ghi_df = dec_merged_for_ghi[[\"datetime\", \"surface_global_irradiance\"]].copy()\r\n",
    "ghi_df = ghi_df.sort_values(\"datetime\").reset_index(drop=True)\r\n",
    "\r\n",
    "# Also keep voltage from the same merged frame\r\n",
    "volt_df = dec_merged_for_ghi[[\"datetime\", \"voltage_avg\"]].copy().sort_values(\"datetime\").reset_index(drop=True)\r\n",
    "\r\n",
    "# ---- Plot per day ----\r\n",
    "os.makedirs(output_dir, exist_ok=True)\r\n",
    "has_actual = \"Power_true_W\" in dfr.columns\r\n",
    "\r\n",
    "# add a local calendar date column for grouping\r\n",
    "dfr[\"date\"] = dfr[\"datetime\"].dt.date\r\n",
    "ghi_df[\"date\"] = ghi_df[\"datetime\"].dt.date\r\n",
    "volt_df[\"date\"] = volt_df[\"datetime\"].dt.date\r\n",
    "\r\n",
    "with PdfPages(pdf_path) as pdf:\r\n",
    "    for date_value, g in dfr.groupby(\"date\", sort=True):\r\n",
    "        g = g.sort_values(\"datetime\")\r\n",
    "\r\n",
    "        # choose which series to fit (prefer actual if we have it)\r\n",
    "        y_fit_col = \"Power_true_W\" if has_actual else \"Power_pred_W\"\r\n",
    "\r\n",
    "        # --- define the polyfit window: poly_start_h to poly_end_h local ---\r\n",
    "        day_start = pd.Timestamp(f\"{date_value} {poly_start_h:02d}:00:00\", tz=tz)\r\n",
    "        day_end   = pd.Timestamp(f\"{date_value} {poly_end_h:02d}:00:00\",   tz=tz)\r\n",
    "\r\n",
    "        gp = g[(g[\"datetime\"] >= day_start) & (g[\"datetime\"] <= day_end)].copy()\r\n",
    "\r\n",
    "        # prepare x as seconds since poly_start_h (helps conditioning)\r\n",
    "        t0 = day_start\r\n",
    "        if not gp.empty:\r\n",
    "            x_fit = (gp[\"datetime\"] - t0).dt.total_seconds().to_numpy()\r\n",
    "            y_fit = gp[y_fit_col].to_numpy()\r\n",
    "        else:\r\n",
    "            x_fit = np.array([])\r\n",
    "            y_fit = np.array([])\r\n",
    "\r\n",
    "        # drop NaNs for fitting\r\n",
    "        valid = np.isfinite(x_fit) & np.isfinite(y_fit)\r\n",
    "        poly_added = False\r\n",
    "        y_poly_eval = None\r\n",
    "\r\n",
    "        if valid.sum() >= (poly_degree + 1):\r\n",
    "            try:\r\n",
    "                coeffs = np.polyfit(x_fit[valid], y_fit[valid], deg=poly_degree)\r\n",
    "                seconds_span = int((day_end - day_start).total_seconds())\r\n",
    "                x_eval = np.linspace(0, seconds_span, 721)  # ~one-minute resolution\r\n",
    "                y_poly_eval = np.poly1d(coeffs)(x_eval)\r\n",
    "                y_poly_eval = np.clip(y_poly_eval, 0, None)  # clamp for plotting\r\n",
    "                dt_eval = day_start + pd.to_timedelta(x_eval, unit=\"s\")\r\n",
    "                poly_added = True\r\n",
    "            except Exception:\r\n",
    "                poly_added = False\r\n",
    "\r\n",
    "        # --- slice GHI and Voltage for the same day (full day) ---\r\n",
    "        ghi_day = ghi_df.loc[ghi_df[\"date\"] == date_value, [\"datetime\", \"surface_global_irradiance\"]]\r\n",
    "        has_ghi = not ghi_day.empty and ghi_day[\"surface_global_irradiance\"].notna().any()\r\n",
    "        if has_ghi:\r\n",
    "            ghi_vals = np.clip(ghi_day[\"surface_global_irradiance\"].to_numpy(), 0, None)\r\n",
    "\r\n",
    "        volt_day = volt_df.loc[volt_df[\"date\"] == date_value, [\"datetime\", \"voltage_avg\"]]\r\n",
    "        has_volt = not volt_day.empty and volt_day[\"voltage_avg\"].notna().any()\r\n",
    "\r\n",
    "        # ---- CLAMP predicted/actual Power for plotting ----\r\n",
    "        pred_clamped = np.clip(g[\"Power_pred_W\"].to_numpy(), 0, None)\r\n",
    "        if has_actual:\r\n",
    "            true_clamped = np.clip(g[\"Power_true_W\"].to_numpy(), 0, None)\r\n",
    "\r\n",
    "        # ========== FIGURE 1: Power (+polyfit) with GHI on right ==========\r\n",
    "        fig1, ax1 = plt.subplots()\r\n",
    "\r\n",
    "        l_pred, = ax1.plot(g[\"datetime\"], pred_clamped, label=\"Predicted Power\")\r\n",
    "        lines = [l_pred]; labels = [\"Predicted Power\"]\r\n",
    "\r\n",
    "        if has_actual:\r\n",
    "            l_act, = ax1.plot(g[\"datetime\"], true_clamped, label=\"Actual Power\")\r\n",
    "            lines.append(l_act); labels.append(\"Actual Power\")\r\n",
    "\r\n",
    "        if poly_added and y_poly_eval is not None:\r\n",
    "            l_poly, = ax1.plot(dt_eval, y_poly_eval, linestyle=\"--\", label=\"Polyfit\")\r\n",
    "            lines.append(l_poly); labels.append(\"Polyfit\")\r\n",
    "\r\n",
    "        if has_ghi:\r\n",
    "            ax2 = ax1.twinx()\r\n",
    "            l_ghi, = ax2.plot(ghi_day[\"datetime\"], ghi_vals, linestyle=\":\", label=\"GHI\")\r\n",
    "            ax2.set_ylabel(\"GHI (W/m²)\")\r\n",
    "            lines.append(l_ghi); labels.append(\"GHI\")\r\n",
    "\r\n",
    "        ax1.set_title(f\"Power & GHI - {date_value} (Australia/Adelaide)\")\r\n",
    "        ax1.set_xlabel(\"Time\")\r\n",
    "        ax1.set_ylabel(\"Power (Kw)\")\r\n",
    "        ax1.grid(True)\r\n",
    "        ax1.legend(lines, labels, loc=\"best\")\r\n",
    "        fig1.autofmt_xdate()\r\n",
    "        fig1.tight_layout()\r\n",
    "\r\n",
    "        png_name_1 = os.path.join(output_dir, f\"power_ghi_{date_value}.png\")\r\n",
    "        fig1.savefig(png_name_1, dpi=150)\r\n",
    "        pdf.savefig(fig1)\r\n",
    "        if show_plots: plt.show()\r\n",
    "        plt.close(fig1)\r\n",
    "\r\n",
    "        # ========== FIGURE 2: Power (+polyfit) with Voltage on right ==========\r\n",
    "        if has_volt:\r\n",
    "            fig2, ax1b = plt.subplots()\r\n",
    "\r\n",
    "            l_pred2, = ax1b.plot(g[\"datetime\"], pred_clamped, label=\"Predicted Power\")\r\n",
    "            lines2 = [l_pred2]; labels2 = [\"Predicted Power\"]\r\n",
    "\r\n",
    "            if has_actual:\r\n",
    "                l_act2, = ax1b.plot(g[\"datetime\"], true_clamped, label=\"Actual Power\")\r\n",
    "                lines2.append(l_act2); labels2.append(\"Actual Power\")\r\n",
    "\r\n",
    "            if poly_added and y_poly_eval is not None:\r\n",
    "                l_poly2, = ax1b.plot(dt_eval, y_poly_eval, linestyle=\"--\", label=\"Polyfit\")\r\n",
    "                lines2.append(l_poly2); labels2.append(\"Polyfit\")\r\n",
    "\r\n",
    "            ax2b = ax1b.twinx()\r\n",
    "            l_volt, = ax2b.plot(volt_day[\"datetime\"], volt_day[\"voltage_avg\"], linestyle=\":\", label=\"Voltage\")\r\n",
    "            ax2b.set_ylabel(\"Voltage (V)\")\r\n",
    "            lines2.append(l_volt); labels2.append(\"Voltage\")\r\n",
    "\r\n",
    "            ax1b.set_title(f\"Power & Voltage - {date_value} (Australia/Adelaide)\")\r\n",
    "            ax1b.set_xlabel(\"Time\")\r\n",
    "            ax1b.set_ylabel(\"Power (Kw)\")\r\n",
    "            ax1b.grid(True)\r\n",
    "            ax1b.legend(lines2, labels2, loc=\"best\")\r\n",
    "            fig2.autofmt_xdate()\r\n",
    "            fig2.tight_layout()\r\n",
    "\r\n",
    "            png_name_2 = os.path.join(output_dir, f\"power_voltage_{date_value}.png\")\r\n",
    "            fig2.savefig(png_name_2, dpi=150)\r\n",
    "            pdf.savefig(fig2)\r\n",
    "            if show_plots: plt.show()\r\n",
    "            plt.close(fig2)\r\n",
    "        else:\r\n",
    "            # Optional: log a note if voltage is missing for this day\r\n",
    "            print(f\"[{date_value}] No voltage data available; skipping Power&Voltage plot.\")\r\n",
    "\r\n",
    "print(f\"Saved daily PNGs to: {os.path.abspath(output_dir)}\")\r\n",
    "print(f\"Saved combined PDF to: {os.path.abspath(pdf_path)}\")\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.7 64-bit",
   "name": "python3137jvsc74a57bd0934f9df0155c18865b31a16060db8114dec29f83c32cc2abbe3f7492f04da8cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "934f9df0155c18865b31a16060db8114dec29f83c32cc2abbe3f7492f04da8cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}